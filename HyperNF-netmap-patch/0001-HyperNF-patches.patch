From 884d614a09489754f6831845331438f38958f2a5 Mon Sep 17 00:00:00 2001
From: Kenichi Yasukata <kenichi.yasukata@neclab.eu>
Date: Wed, 9 Aug 2017 10:08:49 +0100
Subject: [PATCH] HyperNF patches

---
 LINUX/bsd_glue.h             |  39 +++-----
 LINUX/i40e_netmap_linux.h    | 156 +++++++++++++++++++++++++++++++-
 LINUX/netmap_linux.c         |  28 +++++-
 LINUX/xennet_lock.h          | 186 ++++++++++++++++++++++++++++++++++++++
 sys/dev/netmap/netmap.c      |   1 -
 sys/dev/netmap/netmap_kern.h | 210 +++++++++++++++++++++++++++++++++++++------
 sys/dev/netmap/netmap_mem2.c | 205 +++++++++++++++++++++++++++++++++++++++++-
 sys/dev/netmap/netmap_mem2.h |   3 +
 sys/dev/netmap/netmap_vale.c | 126 +++++---------------------
 sys/net/netmap.h             |   3 +-
 10 files changed, 792 insertions(+), 165 deletions(-)
 create mode 100644 LINUX/xennet_lock.h

diff --git a/LINUX/bsd_glue.h b/LINUX/bsd_glue.h
index 909eb13..ac2c710 100644
--- a/LINUX/bsd_glue.h
+++ b/LINUX/bsd_glue.h
@@ -64,6 +64,8 @@
 /*----- support for compiling on older versions of linux -----*/
 #include "netmap_linux_config.h"
 
+#include "xennet_lock.h"
+
 #ifndef NETMAP_LINUX_HAVE_HRTIMER_MODE_REL
 #define HRTIMER_MODE_REL	HRTIMER_REL
 #endif
@@ -166,8 +168,8 @@ struct thread;
 #define bzero(a, len)		memset(a, 0, len)
 
 /* Atomic variables. */
-#define NM_ATOMIC_TEST_AND_SET(p)	test_and_set_bit(0, (p))
-#define NM_ATOMIC_CLEAR(p)		clear_bit(0, (p))
+#define NM_ATOMIC_TEST_AND_SET(p)	(!atomic_cmpset_int((p), 0, 1))
+#define NM_ATOMIC_CLEAR(p)		atomic_store_rel_int((p), 0)
 
 #define NM_ATOMIC_SET(p, v)             atomic_set(p, v)
 #define NM_ATOMIC_INC(p)                atomic_inc(p)
@@ -261,23 +263,8 @@ int linux_netmap_set_channels(struct net_device *, struct ethtool_channels *);
  * We use spin_lock_irqsave() because we use the lock in the
  * (hard) interrupt context.
  */
-typedef struct {
-        spinlock_t      sl;
-        ulong           flags;
-} safe_spinlock_t;
-
-static inline void mtx_lock(safe_spinlock_t *m)
-{
-        spin_lock_irqsave(&(m->sl), m->flags);
-}
-
-static inline void mtx_unlock(safe_spinlock_t *m)
-{
-	ulong flags = ACCESS_ONCE(m->flags);
-        spin_unlock_irqrestore(&(m->sl), flags);
-}
 
-#define mtx_init(a, b, c, d)	spin_lock_init(&((a)->sl))
+#define mtx_init(a, b, c, d)	memset(&((a)->sl), 0, sizeof(rwticket))
 #define mtx_destroy(a)		// XXX spin_lock_destroy(a)
 
 #define mtx_lock_spin(a)	mtx_lock(a)
@@ -288,17 +275,17 @@ static inline void mtx_unlock(safe_spinlock_t *m)
  * Must change to proper rwlock, and then can move the definitions
  * into the main netmap.c file.
  */
-#define BDG_RWLOCK_T		struct rw_semaphore
-#define BDG_RWINIT(b)		init_rwsem(&(b)->bdg_lock)
+#define BDG_RWLOCK_T		safe_spinlock_t
+#define BDG_RWINIT(b)		memset(b, 0, sizeof(safe_spinlock_t))
 #define BDG_RWDESTROY(b)
-#define BDG_WLOCK(b)		down_write(&(b)->bdg_lock)
-#define BDG_WUNLOCK(b)		up_write(&(b)->bdg_lock)
-#define BDG_RLOCK(b)		down_read(&(b)->bdg_lock)
-#define BDG_RUNLOCK(b)		up_read(&(b)->bdg_lock)
-#define BDG_RTRYLOCK(b)		down_read_trylock(&(b)->bdg_lock)
+#define BDG_WLOCK(b)		bdg_wrlock(&(b)->bdg_lock)
+#define BDG_WUNLOCK(b)		bdg_wrunlock(&(b)->bdg_lock)
+#define BDG_RLOCK(b)		bdg_rdlock(&(b)->bdg_lock)
+#define BDG_RUNLOCK(b)		bdg_rdunlock(&(b)->bdg_lock)
+#define BDG_RTRYLOCK(b)		bdg_rdtrylock(&(b)->bdg_lock)
 #define BDG_SET_VAR(lval, p)	((lval) = (p))
 #define BDG_GET_VAR(lval)	(lval)
-#define BDG_FREE(p)		kfree(p)
+#define BDG_FREE(p)		xfree(p)
 
 /*
  * in the malloc/free code we ignore the type
diff --git a/LINUX/i40e_netmap_linux.h b/LINUX/i40e_netmap_linux.h
index d275de9..94b695a 100644
--- a/LINUX/i40e_netmap_linux.h
+++ b/LINUX/i40e_netmap_linux.h
@@ -78,6 +78,155 @@ SYSCTL_INT(_dev_netmap, OID_AUTO, ix_rx_miss,
 SYSCTL_INT(_dev_netmap, OID_AUTO, ix_rx_miss_bufs,
     CTLFLAG_RW, &ix_rx_miss_bufs, 0, "potentially missed rx intr bufs");
 
+#define XENNET_IRQTYPE_I40E 1
+
+static void __xennet_unmap_i40e_sw(struct netmap_adapter *hwna, struct netmap_adapter *swna)
+{
+	if (netmap_xen_ops && netmap_xen_ops->unbind_hwsw) {
+		D("unbind hwsw");
+		netmap_xen_ops->unbind_hwsw(hwna, swna);
+	}
+	if (netmap_xen_ops && netmap_xen_ops->unmap) {
+		D("unmap netmap sw");
+		netmap_xen_ops->unmap(swna);
+	}
+}
+
+static int __xennet_map_i40e_sw(struct netmap_adapter *hwna, struct netmap_adapter *swna)
+{
+	int ret = 0;
+
+	if (netmap_xen_ops && netmap_xen_ops->map) {
+		D("try map netmap sw");
+		ret = netmap_xen_ops->map(swna, false, false);
+		if (ret < 0) {
+			D("failed to map netmap");
+			goto fail0;
+		}
+	}
+	if (netmap_xen_ops && netmap_xen_ops->bind_hwsw) {
+		ret = netmap_xen_ops->bind_hwsw(hwna, swna);
+		if (ret < 0) {
+			D("failed to bind hwsw");
+
+			goto fail1;
+		}
+	}
+
+	return ret;
+fail1:
+	if (netmap_xen_ops && netmap_xen_ops->unmap) {
+		D("unmap netmap sw");
+		netmap_xen_ops->unmap(swna);
+	}
+fail0:
+	return ret;
+}
+
+static void xennet_unmap_i40e(struct netmap_adapter *hwna)
+{
+	struct netmap_adapter *swna;
+
+	if (hwna->na_vp) {
+		swna = &hwna->na_vp->up;
+		__xennet_unmap_i40e_sw(hwna, swna);
+	}
+
+	if (netmap_xen_ops && netmap_xen_ops->unmap_i40e) {
+		D("unmap i40e");
+		netmap_xen_ops->unmap_i40e(hwna);
+	}
+
+	if (netmap_xen_ops && netmap_xen_ops->unmap) {
+		D("unmap netmap hw");
+		netmap_xen_ops->unmap(hwna);
+	}
+}
+
+static int xennet_map_i40e(struct netmap_adapter *hwna)
+{
+	struct netmap_adapter *swna;
+	struct ifnet *ifp = hwna->ifp;
+	struct i40e_netdev_priv *np = netdev_priv(ifp);
+	struct i40e_vsi *vsi = np->vsi;
+	struct i40e_pf *pf = (struct i40e_pf *)vsi->back;
+	struct i40e_ring* txr, *rxr;
+	size_t size_tx_ring = sizeof(struct i40e_ring);
+	int ret = 0, i;
+
+	if (netmap_xen_ops && netmap_xen_ops->map) {
+		D("try map netmap hw");
+		ret = netmap_xen_ops->map(hwna, false, true);
+		if (ret < 0) {
+			D("failed to map netmap");
+			goto out;
+		}
+	}
+
+	if (netmap_xen_ops && netmap_xen_ops->map_i40e) {
+		D("try map i40e");
+		ret = netmap_xen_ops->map_i40e(hwna,
+						pci_resource_start(pf->pdev, 0),
+						pci_resource_len(pf->pdev, 0));
+		if (ret < 0) {
+			D("failed to map i40e");
+			goto fail3;
+		}
+	}
+
+	if (netmap_xen_ops && netmap_xen_ops->map_i40e_ring) {
+		for (i = 0; i < hwna->num_tx_rings; i++) {
+			unsigned long *hung_detected = &vsi->q_vectors[i]->hung_detected;
+			txr = NM_I40E_TX_RING(vsi, i);
+			rxr = NM_I40E_RX_RING(vsi, i);
+			D("%d/%d: try map i40e ring base_queue: %d , queue_index: %d",
+					i,
+					hwna->num_tx_rings,
+					vsi->base_queue, txr->queue_index);
+			ret = netmap_xen_ops->map_i40e_ring(hwna, i, (void *) txr, size_tx_ring,
+							    (void *) rxr, size_tx_ring,
+							    &vsi->q_vectors[i]->tx,
+							    &vsi->q_vectors[i]->rx,
+							    &vsi->base_queue,
+							    &txr->queue_index,
+							    &vsi->base_vector,
+							    (u32 *) hung_detected,
+							    (u32 *) &vsi->state,
+							    (u64 *) &pf->flags);
+			if (ret < 0) {
+				D("failed to map i40e ring");
+				goto fail4;
+			}
+		}
+	}
+
+	if (hwna->na_vp) {
+		swna = &hwna->na_vp->up;
+		ret = __xennet_map_i40e_sw(hwna, swna);
+		if (ret < 0) {
+			D("failed to map i40e sw");
+			goto fail4;
+		}
+	} else {
+		D("This NIC is not attached to any bridge");
+	}
+
+	D("i40e mapping is done");
+	goto out;
+fail4:
+	if (netmap_xen_ops && netmap_xen_ops->unmap_i40e) {
+		D("unmap i40e");
+		netmap_xen_ops->unmap_i40e(hwna);
+	}
+fail3:
+	if (netmap_xen_ops && netmap_xen_ops->unmap) {
+		D("unmap netmap");
+		netmap_xen_ops->unmap(hwna);
+	}
+out:
+	return ret;
+}
+
 #if 0
 static void
 set_crcstrip(struct ixgbe_hw *hw, int onoff)
@@ -193,11 +342,16 @@ i40e_netmap_reg(struct netmap_adapter *na, int onoff)
 	if (onoff) {
 		nm_set_native_flags(na);
 	} else {
+		xennet_unmap_i40e(na);
 		nm_clear_native_flags(na);
 	}
 	if (was_running) {
 		i40e_up(vsi);
 	}
+
+	if (onoff)
+		xennet_map_i40e(na);
+
 	//set_crcstrip(&adapter->hw, onoff); // XXX why twice ?
 
 	clear_bit(__I40E_CONFIG_BUSY, &pf->state);
@@ -424,7 +578,7 @@ i40e_netmap_rxsync(struct netmap_kring *kring, int flags)
 
 	if (!netif_running(ifp))
 		return 0;
-       
+
 	rxr = NM_I40E_RX_RING(vsi, kring->ring_id);
 	if (!rxr)
 		return ENXIO;
diff --git a/LINUX/netmap_linux.c b/LINUX/netmap_linux.c
index 96a8d01..a9b647c 100644
--- a/LINUX/netmap_linux.c
+++ b/LINUX/netmap_linux.c
@@ -969,6 +969,11 @@ linux_netmap_mmap(struct file *f, struct vm_area_struct *vma)
 		vma->vm_private_data = priv;
 		vma->vm_ops = &linux_netmap_mmap_ops;
 	}
+
+	if (netmap_xen_ops && netmap_xen_ops->map) {
+		netmap_xen_ops->map(priv->np_na, true, false);
+	}
+
 	return 0;
 }
 
@@ -1050,8 +1055,14 @@ static int
 linux_netmap_release(struct inode *inode, struct file *file)
 {
 	(void)inode;	/* UNUSED */
-	if (file->private_data)
+	if (file->private_data) {
+		struct netmap_priv_d *priv = file->private_data;
+		if (netmap_xen_ops && netmap_xen_ops->unmap) {
+			if (priv->np_na)
+				netmap_xen_ops->unmap(priv->np_na);
+		}
 		netmap_dtor(file->private_data);
+	}
 	return (0);
 }
 
@@ -2661,6 +2672,21 @@ EXPORT_SYMBOL(netmap_disable_all_rings);
 EXPORT_SYMBOL(netmap_enable_all_rings);
 EXPORT_SYMBOL(netmap_krings_create);
 
+EXPORT_SYMBOL(ifunit_ref);
+EXPORT_SYMBOL(if_rele);
+EXPORT_SYMBOL(netmap_ioctl);
+EXPORT_SYMBOL(netmap_dtor);
+EXPORT_SYMBOL(netmap_priv_new);
+EXPORT_SYMBOL(netmap_global_lock);
+EXPORT_SYMBOL(netmap_mem_get_info);
+EXPORT_SYMBOL(netmap_mem_ofstophys);
+EXPORT_SYMBOL(netmap_mem_if_offset);
+EXPORT_SYMBOL(nm_txsync_prologue);
+EXPORT_SYMBOL(nm_rxsync_prologue);
+struct netmap_xennet_ops *netmap_xen_ops = NULL;
+EXPORT_SYMBOL(netmap_xen_ops);
+EXPORT_SYMBOL(netmap_mem_private_shared_new);
+EXPORT_SYMBOL(netmap_vp_rxsync);
 
 MODULE_AUTHOR("http://info.iet.unipi.it/~luigi/netmap/");
 MODULE_DESCRIPTION("The netmap packet I/O framework");
diff --git a/LINUX/xennet_lock.h b/LINUX/xennet_lock.h
new file mode 100644
index 0000000..bab5725
--- /dev/null
+++ b/LINUX/xennet_lock.h
@@ -0,0 +1,186 @@
+/* Cited from
+ * http://locklessinc.com/articles/locks/
+ * Ticket read-write lock
+ *
+ * Reference: John Mellor-Crummey and Michael Scott,
+ * Scalable Reader-Writer Synchronization for Shared-Memory Multiprocessors
+ */
+
+#ifndef _XENNET_LOCK_H
+#define _XENNET_LOCK_H
+
+#define xn_atomic_xadd(P, V) __sync_fetch_and_add((P), (V))
+#define xn_cmpxchg(P, O, N) __sync_val_compare_and_swap((P), (O), (N))
+#define xn_atomic_inc(P) __sync_add_and_fetch((P), 1)
+#define xn_atomic_dec(P) __sync_add_and_fetch((P), -1) 
+#define xn_atomic_add(P, V) __sync_add_and_fetch((P), (V))
+#define xn_atomic_set_bit(P, V) __sync_or_and_fetch((P), 1<<(V))
+#define xn_atomic_clear_bit(P, V) __sync_and_and_fetch((P), ~(1<<(V)))
+
+typedef union rwticket rwticket;
+
+union rwticket
+{
+	u32 u;
+	u16 us;
+	__extension__ struct
+	{
+		u8 write;
+		u8 read;
+		u8 users;
+		u8 margin;
+	} s;
+};
+
+typedef struct {
+        rwticket    sl;
+        ulong	    flags;
+} safe_spinlock_t;
+
+static void rwticket_wrlock(rwticket *l)
+{
+	u32 me = xn_atomic_xadd(&l->u, (1<<16));
+	u8 val = me >> 16;
+
+	while (val != l->s.write) cpu_relax();
+}
+
+static void rwticket_wrunlock(rwticket *l)
+{
+	rwticket t = *l;
+
+	barrier();
+
+	t.s.write++;
+	t.s.read++;
+
+	*(unsigned short *) l = t.us;
+}
+
+#if defined(__xennet__) || defined(XENNET_LOCK)
+static void rwticket_rdlock(rwticket *l)
+{
+	u32 me = xn_atomic_xadd(&l->u, (1<<16));
+	u8 val = me >> 16;
+
+	while (val != l->s.read) cpu_relax();
+	l->s.read++;
+}
+
+static void rwticket_rdunlock(rwticket *l)
+{
+	xn_atomic_inc(&l->s.write);
+}
+
+static int rwticket_rdtrylock(rwticket *l)
+{
+	u32 me = l->s.users;
+	u32 write = l->s.write;
+	u8 menew = me + 1;
+	u32 cmp = (me << 16) + (me << 8) + write;
+	u32 cmpnew = ((unsigned) menew << 16) + (menew << 8) + write;
+	l->s.margin = 0; // for sure
+
+	if (xn_cmpxchg(&l->u, cmp, cmpnew) == cmp) return 1;
+
+	return 0;
+}
+
+static inline void bdg_rdlock(safe_spinlock_t *m)
+{
+	rwticket_rdlock(&(m->sl));
+}
+
+static inline unsigned int bdg_rdtrylock(safe_spinlock_t *m)
+{
+	return rwticket_rdtrylock(&(m->sl));
+}
+
+static inline void bdg_rdunlock(safe_spinlock_t *m)
+{
+	rwticket_rdunlock(&(m->sl));
+}
+
+#endif
+
+/* irqsave */
+static inline unsigned long rwticket_wrlock_irqsave(rwticket *l)
+{
+	unsigned long flags;
+#if !defined(__xennet__)
+	local_irq_save(flags);
+	preempt_disable();
+#else
+	flags = 0;
+#endif
+	rwticket_wrlock(l);
+
+	return flags;
+}
+
+static inline void rwticket_wrunlock_irqrestore(rwticket *l, unsigned long flags)
+{
+	rwticket_wrunlock(l);
+#if !defined(__xennet__)
+	local_irq_restore(flags);
+	preempt_enable();
+#endif
+}
+
+static inline void bdg_wrlock(safe_spinlock_t *m)
+{
+	rwticket_wrlock(&(m->sl));
+}
+
+static inline void bdg_wrunlock(safe_spinlock_t *m)
+{
+	rwticket_wrunlock(&(m->sl));
+}
+
+static inline void mtx_lock(safe_spinlock_t *m)
+{
+	m->flags = rwticket_wrlock_irqsave(&(m->sl));
+}
+
+static inline void mtx_unlock(safe_spinlock_t *m)
+{
+	ulong flags = ACCESS_ONCE(m->flags);
+	rwticket_wrunlock_irqrestore(&(m->sl), flags);
+}
+
+/*
+ * lock functions for NM_ATOMIC_TEST_AND_SET and NM_ATOMIC_CLEAR
+ */
+
+#define MPLOCKED        "lock ; "
+
+/* Copied from FreeBSD */
+static __inline int
+atomic_cmpset_int(volatile int *dst, int expect, int src)
+{
+	u_char res;
+
+	__asm __volatile(
+	"       " MPLOCKED "            "
+	"       cmpxchgl %3,%1 ;        "
+	"       sete    %0 ;            "
+	"# atomic_cmpset_int"
+	: "=q" (res),                   /* 0 */
+	  "+m" (*dst),                  /* 1 */
+	  "+a" (expect)                 /* 2 */
+	: "r" (src)                     /* 3 */
+	: "memory", "cc");
+	return (res);
+}
+
+#define __compiler_membar()     __asm __volatile(" " : : : "memory")
+/* Copied from FreeBSD */
+static __inline void
+atomic_store_rel_int(volatile int *p, int v)
+{
+
+	__compiler_membar();
+	*p = v;
+}
+
+#endif
diff --git a/sys/dev/netmap/netmap.c b/sys/dev/netmap/netmap.c
index 4f91d61..9621ce8 100644
--- a/sys/dev/netmap/netmap.c
+++ b/sys/dev/netmap/netmap.c
@@ -2326,7 +2326,6 @@ netmap_ioctl(struct netmap_priv_d *priv, u_long cmd, caddr_t data, struct thread
 		}
 
 		break;
-
 #ifdef WITH_VALE
 	case NIOCCONFIG:
 		error = netmap_bdg_config(nmr);
diff --git a/sys/dev/netmap/netmap_kern.h b/sys/dev/netmap/netmap_kern.h
index 8a8b5cf..4015df5 100644
--- a/sys/dev/netmap/netmap_kern.h
+++ b/sys/dev/netmap/netmap_kern.h
@@ -156,7 +156,7 @@ struct hrtimer {
 /* See explanation in nm_os_generic_xmit_frame. */
 #define	GEN_TX_MBUF_IFP(m)	((struct ifnet *)skb_shinfo(m)->destructor_arg)
 
-#define NM_ATOMIC_T	volatile long unsigned int
+#define NM_ATOMIC_T	volatile int
 
 #define NM_MTX_T	struct mutex	/* OS-specific sleepable lock */
 #define NM_MTX_INIT(m)	mutex_init(&(m))
@@ -422,19 +422,21 @@ struct netmap_kring {
 	 */
 	uint64_t	last_reclaim;
 
+	void		*nmif;
+	void		*xen_domain;
+	uint16_t	xen_irq1;
+	uint16_t	xen_irq2;
+	uint16_t	evtchn_port;
+	struct netmap_ring	*xen_ring;
+	struct nm_bdg_fwd	*xen_nkr_ft;
+	uint32_t		*xen_nkr_leases;
+	int (*xen_nm_sync)(struct netmap_kring *kring, int flags);
+	int (*xen_nm_notify)(struct netmap_kring *kring, int flags);
 
-	NM_SELINFO_T	si;		/* poll/select wait queue */
-	NM_LOCK_T	q_lock;		/* protects kring and ring. */
-	NM_ATOMIC_T	nr_busy;	/* prevent concurrent syscalls */
-
-	struct netmap_adapter *na;
+	int (*xen_vp_sync)(struct netmap_kring *kring, int flags);
 
-	/* The following fields are for VALE switch support */
-	struct nm_bdg_fwd *nkr_ft;
-	uint32_t	*nkr_leases;
-#define NR_NOSLOT	((uint32_t)~0)	/* used in nkr_*lease* */
-	uint32_t	nkr_hwlease;
-	uint32_t	nkr_lease_idx;
+	uint32_t	ring_id;	/* kring identifier */
+	enum txrx	tx;		/* kind of ring (tx or rx) */
 
 	/* while nkr_stopped is set, no new [tr]xsync operations can
 	 * be started on this kring.
@@ -444,6 +446,31 @@ struct netmap_kring {
 	 */
 	volatile int nkr_stopped;
 
+	/* The following fields are for VALE switch support */
+	struct nm_bdg_fwd *nkr_ft;
+	uint32_t	*nkr_leases;
+#define NR_NOSLOT	((uint32_t)~0)	/* used in nkr_*lease* */
+	uint32_t	nkr_hwlease;
+	uint32_t	nkr_lease_idx;
+
+//#ifdef WITH_PIPES
+	struct netmap_kring *pipe;	/* if this is a pipe ring,
+					 * pointer to the other end
+					 */
+	struct netmap_ring *save_ring;	/* pointer to hidden rings
+					 * (see netmap_pipe.c for details)
+					 */
+//#endif /* WITH_PIPES */
+
+	NM_ATOMIC_T	nr_busy;	/* prevent concurrent syscalls */
+	NM_LOCK_T	q_lock;		/* protects kring and ring. */
+//-------------------------------------------------------------------------- Shared
+	NM_SELINFO_T	si;		/* poll/select wait queue */
+
+	char name[64];			/* diagnostic */
+
+	struct netmap_adapter *na;
+
 	/* Support for adapters without native netmap support.
 	 * On tx rings we preallocate an array of tx buffers
 	 * (same size as the netmap ring), on rx rings we
@@ -457,10 +484,6 @@ struct netmap_kring {
 
 	uint32_t	users;		/* existing bindings for this ring */
 
-	uint32_t	ring_id;	/* kring identifier */
-	enum txrx	tx;		/* kind of ring (tx or rx) */
-	char name[64];			/* diagnostic */
-
 	/* [tx]sync callback for this kring.
 	 * The default nm_kring_create callback (netmap_krings_create)
 	 * sets the nm_sync callback of each hardware tx(rx) kring to
@@ -475,19 +498,13 @@ struct netmap_kring {
 	int (*nm_sync)(struct netmap_kring *kring, int flags);
 	int (*nm_notify)(struct netmap_kring *kring, int flags);
 
-#ifdef WITH_PIPES
-	struct netmap_kring *pipe;	/* if this is a pipe ring,
-					 * pointer to the other end
-					 */
-	struct netmap_ring *save_ring;	/* pointer to hidden rings
-       					 * (see netmap_pipe.c for details)
-					 */
-#endif /* WITH_PIPES */
-
 #ifdef WITH_VALE
 	int (*save_notify)(struct netmap_kring *kring, int flags);
 #endif
 
+	struct task_struct *hwtsk;
+	NM_SELINFO_T hwwq;
+
 #ifdef WITH_MONITOR
 	/* array of krings that are monitoring this kring */
 	struct netmap_kring **monitors;
@@ -686,6 +703,7 @@ struct netmap_adapter {
 	 * the generic netmap functions.
 	 */
 	struct ifnet *ifp; /* adapter is ifp->if_softc */
+	struct ifnet *ifp2;
 
 	/*---- callbacks for this netmap adapter -----*/
 	/*
@@ -725,6 +743,7 @@ struct netmap_adapter {
 	 *      kring->nm_notify.
 	 *      Return values are the same as for netmap_rx_irq().
 	 */
+	int (*xennet_map)(struct netmap_adapter *);
 	void (*nm_dtor)(struct netmap_adapter *);
 
 	int (*nm_register)(struct netmap_adapter *, int onoff);
@@ -792,6 +811,8 @@ struct netmap_adapter {
 	/* Offset of ethernet header for each packet. */
 	u_int virt_hdr_len;
 
+	void *xen_be;
+
 	char name[64];
 };
 
@@ -1398,7 +1419,7 @@ struct netmap_bdg_ops {
 u_int netmap_bdg_learning(struct nm_bdg_fwd *ft, uint8_t *dst_ring,
 		struct netmap_vp_adapter *);
 
-#define	NM_BRIDGES		8	/* number of bridges */
+#define	NM_BRIDGES		64	/* number of bridges */
 #define	NM_BDG_MAXPORTS		254	/* up to 254 */
 #define	NM_BDG_BROADCAST	NM_BDG_MAXPORTS
 #define	NM_BDG_NOPORT		(NM_BDG_MAXPORTS+1)
@@ -1717,6 +1738,8 @@ netmap_idx_k2n(struct netmap_kring *kr, int idx)
 struct lut_entry {
 	void *vaddr;		/* virtual address. */
 	vm_paddr_t paddr;	/* physical address. */
+	void *xvaddr;		/* Xen virtual address. */
+	xen_pfn_t gfn;
 };
 
 struct netmap_obj_pool;
@@ -2065,4 +2088,139 @@ bool netmap_pt_guest_rxsync(struct ptnet_ring *ptring, struct netmap_kring *krin
 			    int flags);
 #endif /* WITH_PTNETMAP_GUEST */
 
+/*
+ * system parameters (most of them in netmap_kern.h)
+ * NM_NAME	prefix for switch port names, default "vale"
+ * NM_BDG_MAXPORTS	number of ports
+ * NM_BRIDGES	max number of switches in the system.
+ *	XXX should become a sysctl or tunable
+ *
+ * Switch ports are named valeX:Y where X is the switch name and Y
+ * is the port. If Y matches a physical interface name, the port is
+ * connected to a physical device.
+ *
+ * Unlike physical interfaces, switch ports use their own memory region
+ * for rings and buffers.
+ * The virtual interfaces use per-queue lock instead of core lock.
+ * In the tx loop, we aggregate traffic in batches to make all operations
+ * faster. The batch size is bridge_batch.
+ */
+#define NM_BDG_MAXRINGS		16	/* XXX unclear how many. */
+#define NM_BDG_MAXSLOTS		4096	/* XXX same as above */
+#define NM_BRIDGE_RINGSIZE	1024	/* in the device */
+#define NM_BDG_HASH		1024	/* forwarding table entries */
+#define NM_BDG_BATCH		1024	/* entries in the forwarding buffer */
+#define NM_MULTISEG		64	/* max size of a chain of bufs */
+/* actual size of the tables */
+#define NM_BDG_BATCH_MAX	(NM_BDG_BATCH + NM_MULTISEG)
+/* NM_FT_NULL terminates a list of slots in the ft */
+#define NM_FT_NULL		NM_BDG_BATCH_MAX
+
+/*
+ * For each output interface, nm_bdg_q is used to construct a list.
+ * bq_len is the number of output buffers (we can have coalescing
+ * during the copy).
+ */
+struct nm_bdg_q {
+	uint16_t bq_head;
+	uint16_t bq_tail;
+	uint32_t bq_len;	/* number of buffers */
+};
+
+/* XXX revise this */
+struct nm_hash_ent {
+	uint64_t	mac;	/* the top 2 bytes are the epoch */
+	uint64_t	ports;
+};
+
+/*
+ * nm_bridge is a descriptor for a VALE switch.
+ * Interfaces for a bridge are all in bdg_ports[].
+ * The array has fixed size, an empty entry does not terminate
+ * the search, but lookups only occur on attach/detach so we
+ * don't mind if they are slow.
+ *
+ * The bridge is non blocking on the transmit ports: excess
+ * packets are dropped if there is no room on the output port.
+ *
+ * bdg_lock protects accesses to the bdg_ports array.
+ * This is a rw lock (or equivalent).
+ */
+struct nm_bridge {
+	/* XXX what is the proper alignment/layout ? */
+	BDG_RWLOCK_T	bdg_lock;	/* protects bdg_ports */
+	int		bdg_namelen;
+	uint32_t	bdg_active_ports; /* 0 means free */
+	char		bdg_basename[IFNAMSIZ];
+
+	/* Indexes of active ports (up to active_ports)
+	 * and all other remaining ports.
+	 */
+	uint8_t		bdg_port_index[NM_BDG_MAXPORTS];
+
+	/* the forwarding table, MAC+ports.
+	 * XXX should be changed to an argument to be passed to
+	 * the lookup function, and allocated on attach
+	 */
+	struct nm_hash_ent ht[NM_BDG_HASH];
+
+	/*
+	 * The function to decide the destination port.
+	 * It returns either of an index of the destination port,
+	 * NM_BDG_BROADCAST to broadcast this packet, or NM_BDG_NOPORT not to
+	 * forward this packet.  ring_nr is the source ring index, and the
+	 * function may overwrite this value to forward this packet to a
+	 * different ring index.
+	 * This function must be set by netmap_bdg_ctl().
+	 */
+	struct netmap_bdg_ops bdg_ops;
+	struct netmap_bdg_ops xen_bdg_ops;
+
+	void *xen_bdg_ports[NM_BDG_MAXPORTS];
+//------------------------------------------------------------------- Shared
+
+	struct netmap_vp_adapter *bdg_ports[NM_BDG_MAXPORTS];
+
+#ifdef CONFIG_NET_NS
+	struct net *ns;
+#endif /* CONFIG_NET_NS */
+};
+
+typedef int (*map_fn_t)(struct netmap_adapter *, bool, bool);
+typedef void (*unmap_fn_t)(struct netmap_adapter *);
+typedef int (*attach_fn_t)(struct nm_bridge *, struct netmap_vp_adapter *);
+typedef void (*detach_fn_t)(struct netmap_vp_adapter *);
+typedef int (*bind_hwsw_fn_t)(struct netmap_adapter *, struct netmap_adapter *);
+typedef void (*unbind_hwsw_fn_t)(struct netmap_adapter *, struct netmap_adapter *);
+typedef int (*expose_irq_t)(struct netmap_adapter *na, unsigned int irq, unsigned int irqtype, unsigned int ring_id);
+typedef int (*unexpose_irq_t)(struct netmap_adapter *na, unsigned int irq, unsigned int irqtype, unsigned int ring_id);
+typedef int (*map_i40e_fn_t)(struct netmap_adapter *na,
+			     resource_size_t dev_hw_addr, resource_size_t resouce_size);
+typedef int (*map_i40e_ring_fn_t)(struct netmap_adapter *na, int rid, void *tx_ring, size_t size_tx_ring,
+				    void *rx_ring, size_t size_rx_ring,
+				    void *tx_rc, void *rx_rc,
+				    u16 *base_queue_ptr, u16 *queue_index_ptr,
+				    u32 *base_vector_ptr,
+				    u32 *hung_detected,
+				    u32 *state,
+				    u64 *flags);
+typedef void (*unmap_i40e_fn_t)(struct netmap_adapter *na);
+struct netmap_xennet_ops {
+	map_fn_t map;
+	unmap_fn_t unmap;
+	attach_fn_t bdg_attach;
+	detach_fn_t bdg_detach;
+	bind_hwsw_fn_t bind_hwsw;
+	unbind_hwsw_fn_t unbind_hwsw;
+	expose_irq_t expose_irq;
+	unexpose_irq_t unexpose_irq;
+	map_i40e_fn_t map_i40e;
+	map_i40e_ring_fn_t map_i40e_ring;
+	unmap_i40e_fn_t unmap_i40e;
+};
+
+extern struct netmap_xennet_ops *netmap_xen_ops;
+
+int netmap_vp_rxsync(struct netmap_kring *kring, int flags);
+
 #endif /* _NET_NETMAP_KERN_H_ */
diff --git a/sys/dev/netmap/netmap_mem2.c b/sys/dev/netmap/netmap_mem2.c
index 93096f8..6c4314c 100644
--- a/sys/dev/netmap/netmap_mem2.c
+++ b/sys/dev/netmap/netmap_mem2.c
@@ -67,7 +67,8 @@ MALLOC_DEFINE(M_NETMAP, "netmap", "Network memory map");
 #ifdef _WIN32_USE_SMALL_GENERIC_DEVICES_MEMORY
 #define NETMAP_BUF_MAX_NUM  8*4096      /* if too big takes too much time to allocate */
 #else
-#define NETMAP_BUF_MAX_NUM 20*4096*2	/* large machine */
+//#define NETMAP_BUF_MAX_NUM 20*4096*2	/* large machine */
+#define NETMAP_BUF_MAX_NUM 4096*8	/* large machine */
 #endif
 
 #define NETMAP_POOL_MAX_NAMSZ	32
@@ -415,6 +416,20 @@ static struct netmap_obj_params netmap_min_priv_params[NETMAP_POOLS_NR] = {
 	},
 };
 
+static struct netmap_obj_params netmap_min_priv_shared_params[NETMAP_POOLS_NR] = {
+	[NETMAP_IF_POOL] = {
+		.size = 1024,
+		.num  = 2,
+	},
+	[NETMAP_RING_POOL] = {
+		.size = 5*PAGE_SIZE,
+		.num  = 4,
+	},
+	[NETMAP_BUF_POOL] = {
+		.size = 2048,
+		.num  = 4098,
+	},
+};
 
 /*
  * nm_mem is the memory allocator used for all physical interfaces
@@ -492,6 +507,37 @@ static const struct netmap_mem_d nm_blueprint = {
 	.ops = &netmap_mem_private_ops
 };
 
+extern struct netmap_mem_ops netmap_mem_private_shared_ops; /* forward */
+static const struct netmap_mem_d nm_blueprint_shared = {
+	.pools = {
+		[NETMAP_IF_POOL] = {
+			.name 	= "%s_if",
+			.objminsize = sizeof(struct netmap_if),
+			.objmaxsize = 4096,
+			.nummin     = 1,
+			.nummax	    = 100,
+		},
+		[NETMAP_RING_POOL] = {
+			.name 	= "%s_ring",
+			.objminsize = sizeof(struct netmap_ring),
+			.objmaxsize = 32*PAGE_SIZE,
+			.nummin     = 2,
+			.nummax	    = 1024,
+		},
+		[NETMAP_BUF_POOL] = {
+			.name	= "%s_buf",
+			.objminsize = 64,
+			.objmaxsize = 65536,
+			.nummin     = 4,
+			.nummax	    = 1000000, /* one million! */
+		},
+	},
+
+	.flags = NETMAP_MEM_PRIVATE,
+
+	.ops = &netmap_mem_private_shared_ops
+};
+
 /* memory allocator related sysctls */
 
 #define STRINGIFY(x) #x
@@ -1085,7 +1131,7 @@ netmap_reset_obj_allocator(struct netmap_obj_pool *p)
 		}
 		bzero(p->lut, sizeof(struct lut_entry) * p->objtotal);
 #ifdef linux
-		vfree(p->lut);
+		kfree(p->lut);
 #else
 		free(p->lut, M_NETMAP);
 #endif
@@ -1204,13 +1250,15 @@ nm_alloc_lut(u_int nobj)
 	size_t n = sizeof(struct lut_entry) * nobj;
 	struct lut_entry *lut;
 #ifdef linux
-	lut = vmalloc(n);
+	lut = kzalloc(n, GFP_KERNEL);
 #else
 	lut = malloc(n, M_NETMAP, M_NOWAIT | M_ZERO);
 #endif
 	return lut;
 }
 
+
+#include <xen/page.h>
 /* call with NMA_LOCK held */
 static int
 netmap_finalize_obj_allocator(struct netmap_obj_pool *p)
@@ -1242,6 +1290,15 @@ netmap_finalize_obj_allocator(struct netmap_obj_pool *p)
 	 * Allocate clusters, init pointers and bitmap
 	 */
 
+	if (xen_pv_domain()) {
+		if (!xen_feature(XENFEAT_auto_translated_physmap))
+			D("Xen PV domain");
+		else
+			D("Xen HVM domain");
+	} else {
+		D("Baremetal");
+	}
+
 	n = p->_clustsize;
 	for (i = 0; i < (int)p->objtotal;) {
 		int lim = i + p->_clustentries;
@@ -1293,7 +1350,14 @@ netmap_finalize_obj_allocator(struct netmap_obj_pool *p)
 		for (; i < lim; i++, clust += p->_objsize) {
 			p->bitmap[ (i>>5) ] |=  ( 1 << (i & 31) );
 			p->lut[i].vaddr = clust;
-			p->lut[i].paddr = vtophys(clust);
+			if (xen_pv_domain()) {
+				if (!xen_feature(XENFEAT_auto_translated_physmap))
+					p->lut[i].paddr = virt_to_machine(clust).maddr;
+				else
+					p->lut[i].paddr = vtophys(clust);
+			} else {
+				p->lut[i].paddr = vtophys(clust);
+			}
 		}
 	}
 	p->objfree = p->objtotal;
@@ -1452,6 +1516,36 @@ error:
 }
 
 
+static void
+netmap_mem_private_shared_delete(struct netmap_mem_d *nmd)
+{
+	int last;
+
+	if (nmd == NULL)
+		return;
+	if (netmap_verbose)
+		D("deleting %p", nmd);
+
+	NMA_LOCK(nmd);
+	last = (--nmd->refcount == 0);
+	NM_DBG_REFC(nmd, func, line);
+	NMA_UNLOCK(nmd);
+
+	if (!last) {
+		D("There are still users %u", last);
+		return;
+	}
+	D("Delete nm_mem");
+
+	if (nmd->active > 0)
+		D("bug: deleting mem allocator with active=%d!", nmd->active);
+
+	nm_mem_release_id(nmd);
+	if (netmap_verbose)
+		D("done deleting %p", nmd);
+	NMA_LOCK_DESTROY(nmd);
+	free(nmd, M_DEVBUF);
+}
 
 static void
 netmap_mem_private_delete(struct netmap_mem_d *nmd)
@@ -1497,6 +1591,95 @@ netmap_mem_private_deref(struct netmap_mem_d *nmd)
 }
 
 
+struct netmap_mem_d *
+netmap_mem_private_shared_new(const char *name, u_int txr, u_int txd,
+	u_int rxr, u_int rxd, u_int extra_bufs, u_int npipes, int *perr)
+{
+	struct netmap_mem_d *d = NULL;
+	struct netmap_obj_params p[NETMAP_POOLS_NR];
+	int i, err;
+	u_int v, maxd;
+
+	d = malloc(sizeof(struct netmap_mem_d),
+		   M_DEVBUF, M_NOWAIT | M_ZERO);
+	if (d == NULL) {
+		err = ENOMEM;
+		goto error;
+	}
+
+	*d = nm_blueprint_shared;
+
+	err = nm_mem_assign_id(d);
+	if (err)
+		goto error;
+
+	/* account for the fake host rings */
+	txr++;
+	rxr++;
+
+	/* copy the min values */
+	for (i = 0; i < NETMAP_POOLS_NR; i++) {
+		p[i] = netmap_min_priv_shared_params[i];
+	}
+
+	/* possibly increase them to fit user request */
+	v = sizeof(struct netmap_if) + sizeof(ssize_t) * (txr + rxr);
+	if (p[NETMAP_IF_POOL].size < v)
+		p[NETMAP_IF_POOL].size = v;
+	v = 2 + 4 * npipes;
+	if (p[NETMAP_IF_POOL].num < v)
+		p[NETMAP_IF_POOL].num = v;
+	maxd = (txd > rxd) ? txd : rxd;
+	v = sizeof(struct netmap_ring) + sizeof(struct netmap_slot) * maxd;
+	if (p[NETMAP_RING_POOL].size < v)
+		p[NETMAP_RING_POOL].size = v;
+	/* each pipe endpoint needs two tx rings (1 normal + 1 host, fake)
+         * and two rx rings (again, 1 normal and 1 fake host)
+         */
+	v = txr + rxr + 8 * npipes;
+	if (p[NETMAP_RING_POOL].num < v)
+		p[NETMAP_RING_POOL].num = v;
+	/* for each pipe we only need the buffers for the 4 "real" rings.
+         * On the other end, the pipe ring dimension may be different from
+         * the parent port ring dimension. As a compromise, we allocate twice the
+         * space actually needed if the pipe rings were the same size as the parent rings
+         */
+	v = (4 * npipes + rxr) * rxd + (4 * npipes + txr) * txd + 2 + extra_bufs;
+		/* the +2 is for the tx and rx fake buffers (indices 0 and 1) */
+	if (p[NETMAP_BUF_POOL].num < v)
+		p[NETMAP_BUF_POOL].num = v;
+
+	//if (netmap_verbose)
+		D("req if %d*%d ring %d*%d buf %d*%d",
+			p[NETMAP_IF_POOL].num,
+			p[NETMAP_IF_POOL].size,
+			p[NETMAP_RING_POOL].num,
+			p[NETMAP_RING_POOL].size,
+			p[NETMAP_BUF_POOL].num,
+			p[NETMAP_BUF_POOL].size);
+
+	for (i = 0; i < NETMAP_POOLS_NR; i++) {
+		snprintf(d->pools[i].name, NETMAP_POOL_MAX_NAMSZ,
+				nm_blueprint.pools[i].name,
+				name);
+		err = netmap_config_obj_allocator(&d->pools[i],
+				p[i].num, p[i].size);
+		if (err)
+			goto error;
+	}
+
+	d->flags &= ~NETMAP_MEM_FINALIZED;
+
+	NMA_LOCK_INIT(d);
+
+	return d;
+error:
+	netmap_mem_private_shared_delete(d);
+	if (perr)
+		*perr = err;
+	return NULL;
+}
+
 /*
  * allocator for private memory
  */
@@ -1909,6 +2092,20 @@ struct netmap_mem_ops netmap_mem_private_ops = {
 	.nmd_rings_create = netmap_mem2_rings_create,
 	.nmd_rings_delete = netmap_mem2_rings_delete
 };
+struct netmap_mem_ops netmap_mem_private_shared_ops = {
+	.nmd_get_lut = netmap_mem2_get_lut,
+	.nmd_get_info = netmap_mem2_get_info,
+	.nmd_ofstophys = netmap_mem2_ofstophys,
+	.nmd_config = netmap_mem_private_config,
+	.nmd_finalize = netmap_mem_private_finalize,
+	.nmd_deref = netmap_mem_private_deref,
+	.nmd_if_offset = netmap_mem2_if_offset,
+	.nmd_delete = netmap_mem_private_shared_delete,
+	.nmd_if_new = netmap_mem2_if_new,
+	.nmd_if_delete = netmap_mem2_if_delete,
+	.nmd_rings_create = netmap_mem2_rings_create,
+	.nmd_rings_delete = netmap_mem2_rings_delete
+};
 
 #ifdef WITH_PTNETMAP_GUEST
 struct mem_pt_if {
diff --git a/sys/dev/netmap/netmap_mem2.h b/sys/dev/netmap/netmap_mem2.h
index 5e585e2..7c57ec0 100644
--- a/sys/dev/netmap/netmap_mem2.h
+++ b/sys/dev/netmap/netmap_mem2.h
@@ -136,6 +136,9 @@ ssize_t    netmap_mem_if_offset(struct netmap_mem_d *, const void *vaddr);
 struct netmap_mem_d* netmap_mem_private_new(const char *name,
 	u_int txr, u_int txd, u_int rxr, u_int rxd, u_int extra_bufs, u_int npipes,
 	int* error);
+struct netmap_mem_d* netmap_mem_private_shared_new(const char *name,
+	u_int txr, u_int txd, u_int rxr, u_int rxd, u_int extra_bufs, u_int npipes,
+	int* error);
 void	   netmap_mem_delete(struct netmap_mem_d *);
 
 //#define NM_DEBUG_MEM_PUTGET 1
diff --git a/sys/dev/netmap/netmap_vale.c b/sys/dev/netmap/netmap_vale.c
index 9e290f1..7ed9466 100644
--- a/sys/dev/netmap/netmap_vale.c
+++ b/sys/dev/netmap/netmap_vale.c
@@ -94,6 +94,7 @@ __FBSDID("$FreeBSD: head/sys/dev/netmap/netmap.c 257176 2013-10-26 17:58:36Z gle
 
 #elif defined(linux)
 
+#define XENNET_LOCK
 #include "bsd_glue.h"
 
 #elif defined(__APPLE__)
@@ -121,35 +122,6 @@ __FBSDID("$FreeBSD: head/sys/dev/netmap/netmap.c 257176 2013-10-26 17:58:36Z gle
 #ifdef WITH_VALE
 
 /*
- * system parameters (most of them in netmap_kern.h)
- * NM_NAME	prefix for switch port names, default "vale"
- * NM_BDG_MAXPORTS	number of ports
- * NM_BRIDGES	max number of switches in the system.
- *	XXX should become a sysctl or tunable
- *
- * Switch ports are named valeX:Y where X is the switch name and Y
- * is the port. If Y matches a physical interface name, the port is
- * connected to a physical device.
- *
- * Unlike physical interfaces, switch ports use their own memory region
- * for rings and buffers.
- * The virtual interfaces use per-queue lock instead of core lock.
- * In the tx loop, we aggregate traffic in batches to make all operations
- * faster. The batch size is bridge_batch.
- */
-#define NM_BDG_MAXRINGS		16	/* XXX unclear how many. */
-#define NM_BDG_MAXSLOTS		4096	/* XXX same as above */
-#define NM_BRIDGE_RINGSIZE	1024	/* in the device */
-#define NM_BDG_HASH		1024	/* forwarding table entries */
-#define NM_BDG_BATCH		1024	/* entries in the forwarding buffer */
-#define NM_MULTISEG		64	/* max size of a chain of bufs */
-/* actual size of the tables */
-#define NM_BDG_BATCH_MAX	(NM_BDG_BATCH + NM_MULTISEG)
-/* NM_FT_NULL terminates a list of slots in the ft */
-#define NM_FT_NULL		NM_BDG_BATCH_MAX
-
-
-/*
  * bridge_batch is set via sysctl to the max batch size to be
  * used in the bridge. The actual value may be larger as the
  * last packet in the block may overflow the size.
@@ -163,73 +135,8 @@ SYSEND;
 static int netmap_vp_create(struct nmreq *, struct ifnet *, struct netmap_vp_adapter **);
 static int netmap_vp_reg(struct netmap_adapter *na, int onoff);
 static int netmap_bwrap_reg(struct netmap_adapter *, int onoff);
-
-/*
- * For each output interface, nm_bdg_q is used to construct a list.
- * bq_len is the number of output buffers (we can have coalescing
- * during the copy).
- */
-struct nm_bdg_q {
-	uint16_t bq_head;
-	uint16_t bq_tail;
-	uint32_t bq_len;	/* number of buffers */
-};
-
-/* XXX revise this */
-struct nm_hash_ent {
-	uint64_t	mac;	/* the top 2 bytes are the epoch */
-	uint64_t	ports;
-};
-
-/*
- * nm_bridge is a descriptor for a VALE switch.
- * Interfaces for a bridge are all in bdg_ports[].
- * The array has fixed size, an empty entry does not terminate
- * the search, but lookups only occur on attach/detach so we
- * don't mind if they are slow.
- *
- * The bridge is non blocking on the transmit ports: excess
- * packets are dropped if there is no room on the output port.
- *
- * bdg_lock protects accesses to the bdg_ports array.
- * This is a rw lock (or equivalent).
- */
-struct nm_bridge {
-	/* XXX what is the proper alignment/layout ? */
-	BDG_RWLOCK_T	bdg_lock;	/* protects bdg_ports */
-	int		bdg_namelen;
-	uint32_t	bdg_active_ports; /* 0 means free */
-	char		bdg_basename[IFNAMSIZ];
-
-	/* Indexes of active ports (up to active_ports)
-	 * and all other remaining ports.
-	 */
-	uint8_t		bdg_port_index[NM_BDG_MAXPORTS];
-
-	struct netmap_vp_adapter *bdg_ports[NM_BDG_MAXPORTS];
-
-
-	/*
-	 * The function to decide the destination port.
-	 * It returns either of an index of the destination port,
-	 * NM_BDG_BROADCAST to broadcast this packet, or NM_BDG_NOPORT not to
-	 * forward this packet.  ring_nr is the source ring index, and the
-	 * function may overwrite this value to forward this packet to a
-	 * different ring index.
-	 * This function must be set by netmap_bdg_ctl().
-	 */
-	struct netmap_bdg_ops bdg_ops;
-
-	/* the forwarding table, MAC+ports.
-	 * XXX should be changed to an argument to be passed to
-	 * the lookup function, and allocated on attach
-	 */
-	struct nm_hash_ent ht[NM_BDG_HASH];
-
-#ifdef CONFIG_NET_NS
-	struct net *ns;
-#endif /* CONFIG_NET_NS */
-};
+static int netmap_vp_txsync(struct netmap_kring *kring, int flags);
+int netmap_vp_rxsync(struct netmap_kring *kring, int flags);
 
 const char*
 netmap_bdg_name(struct netmap_vp_adapter *vp)
@@ -460,8 +367,10 @@ netmap_bdg_detach_common(struct nm_bridge *b, int hw, int sw)
 	if (b->bdg_ops.dtor)
 		b->bdg_ops.dtor(b->bdg_ports[s_hw]);
 	b->bdg_ports[s_hw] = NULL;
+	b->xen_bdg_ports[s_hw] = NULL;
 	if (s_sw >= 0) {
 		b->bdg_ports[s_sw] = NULL;
+		b->xen_bdg_ports[s_sw] = NULL;
 	}
 	memcpy(b->bdg_port_index, tmp, sizeof(tmp));
 	b->bdg_active_ports = lim;
@@ -1351,10 +1260,10 @@ nm_bdg_preflush(struct netmap_kring *kring, u_int end)
 	 * attached to a user process) or with a trylock otherwise (NICs).
 	 */
 	ND("wait rlock for %d packets", ((j > end ? lim+1 : 0) + end) - j);
-	if (na->up.na_flags & NAF_BDG_MAYSLEEP)
+	//if (na->up.na_flags & NAF_BDG_MAYSLEEP)
 		BDG_RLOCK(b);
-	else if (!BDG_RTRYLOCK(b))
-		return 0;
+	//else if (!BDG_RTRYLOCK(b))
+	//	return 0;
 	ND(5, "rlock acquired for %d packets", ((j > end ? lim+1 : 0) + end) - j);
 	ft = kring->nkr_ft;
 
@@ -1789,7 +1698,8 @@ nm_bdg_flush(struct nm_bdg_fwd *ft, u_int n, struct netmap_vp_adapter *na,
 
 		ND(5, "pass 2 dst %d is %x %s",
 			i, d_i, is_vp ? "virtual" : "nic/host");
-		dst_nr = d_i & (NM_BDG_MAXRINGS-1);
+		//dst_nr = d_i & (NM_BDG_MAXRINGS-1);
+		dst_nr = (smp_processor_id()+1) & (NM_BDG_MAXRINGS-1);
 		nrings = dst_na->up.num_rx_rings;
 		if (dst_nr >= nrings)
 			dst_nr = dst_nr % nrings;
@@ -2054,7 +1964,7 @@ done:
  * but we must acquire the queue's lock to protect against
  * writers on the same queue.
  */
-static int
+int
 netmap_vp_rxsync(struct netmap_kring *kring, int flags)
 {
 	int n;
@@ -2144,10 +2054,16 @@ netmap_vp_create(struct nmreq *nmr, struct ifnet *ifp, struct netmap_vp_adapter
 	na->nm_krings_create = netmap_vp_krings_create;
 	na->nm_krings_delete = netmap_vp_krings_delete;
 	na->nm_dtor = netmap_vp_dtor;
-	na->nm_mem = netmap_mem_private_new(na->name,
-			na->num_tx_rings, na->num_tx_desc,
-			na->num_rx_rings, na->num_rx_desc,
-			nmr->nr_arg3, npipes, &error);
+	if (nmr->nm_mem) {
+		D("We share the same nm_mem %p", nmr->nm_mem);
+		na->nm_mem = (struct netmap_mem_d *) nmr->nm_mem;
+		netmap_mem_get(na->nm_mem);
+	} else {
+		na->nm_mem = netmap_mem_private_new(na->name,
+				na->num_tx_rings, na->num_tx_desc,
+				na->num_rx_rings, na->num_rx_desc,
+				nmr->nr_arg3, npipes, &error);
+	}
 	if (na->nm_mem == NULL)
 		goto err;
 	na->nm_bdg_attach = netmap_vp_bdg_attach;
diff --git a/sys/net/netmap.h b/sys/net/netmap.h
index a9a8756..4ef16c3 100644
--- a/sys/net/netmap.h
+++ b/sys/net/netmap.h
@@ -362,7 +362,7 @@ struct netmap_if {
 	 * The area is filled up by the kernel on NIOCREGIF,
 	 * and then only read by userspace code.
 	 */
-	const ssize_t	ring_ofs[0];
+	const uint64_t	ring_ofs[0];
 };
 
 
@@ -533,6 +533,7 @@ struct nmreq {
 	uint32_t	nr_flags;
 	/* various modes, extends nr_ringid */
 	uint32_t	spare2[1];
+	void *nm_mem;
 };
 
 #define NR_REG_MASK		0xf /* values for nr_flags */
-- 
2.1.4

